{
  "topic": "Machine Learning",
  "experience_level": "beginner",
  "time_commitment": "5-10 hours per week",
  "learning_goals": "",
  "study_plan": {
    "topic": "Machine Learning",
    "experience_level": "beginner",
    "time_commitment": "5-10 hours per week",
    "learning_goals": "",
    "total_weeks": 6,
    "weekly_goals": [
      {
        "week_number": 1,
        "title": "Introduction to Machine Learning",
        "description": "Understanding the fundamentals of ML, types of ML, and its applications.",
        "resources": [
          {
            "title": "Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn",
            "description": "Artificial Intelligence Engineer (IBM) ...",
            "url": "https://www.youtube.com/watch?v=ukzFI9rgwfU",
            "resource_type": "youtube_video",
            "duration": "PT7M52S",
            "difficulty": null,
            "tags": [
              "2022",
              "2023",
              "ML",
              "Machine Learning",
              "basics of machine learning",
              "cloud and machine learning",
              "fundamentals of machine learning",
              "gcp maching learning",
              "getting started with machine learning",
              "introduction to machine learning",
              "machine learning",
              "machine learning basics",
              "machine learning for beginners",
              "machine learning tutorial for beginners",
              "machine learning with gcp",
              "simplilearn",
              "simplilearn machine learning",
              "what is machine learning"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Introduction To Machine Learning ll Machine Learning Course Explained With RealLife Examples (Hindi)",
            "description": "LIVE ULTIMATE DATA BOOTCAMP   https://www.5minutesengineering.com/ Myself Shridhar Mankar a Engineer l YouTuber l ...",
            "url": "https://www.youtube.com/watch?v=Y4qO9unerGs",
            "resource_type": "youtube_video",
            "duration": "PT12M1S",
            "difficulty": null,
            "tags": [
              "machine learning course",
              "machine learning"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Machine Learning Introduction | Machine Learning Tutorial | Simplilearn",
            "description": "Artificial Intelligence Engineer (IBM) ...",
            "url": "https://www.youtube.com/watch?v=seG9J49bBYI",
            "resource_type": "youtube_video",
            "duration": "PT9M2S",
            "difficulty": null,
            "tags": [
              "simplilearn",
              "training",
              "tutorial",
              "certification",
              "course",
              "curriculum",
              "Free resources",
              "Machine Learning Introduction",
              "machine learning",
              "machine learning tutorial",
              "machine learning tutorial for beginners",
              "machine learning python",
              "machine learning algorithms",
              "machine learning projects",
              "2017",
              "regression",
              "clustering",
              "classification",
              "Prediction",
              "Spark Machine"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Theano-Tutorials",
            "description": "Bare bones introduction to machine learning from linear regression to convolutional neural networks using Theano.",
            "url": "https://github.com/Newmu/Theano-Tutorials",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "hol-azure-machine-learning",
            "description": "Introduction to Machine Learning and Azure Machine Learning Services. Hands on labs to show Azure Machine Learning features, developing experiments, feature engineering, R and Python Scripting, Production stage, publishing models as web service, RRS and BES usage",
            "url": "https://github.com/microsoft/hol-azure-machine-learning",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Define machine learning and its key concepts.",
          "Differentiate between supervised, unsupervised, and reinforcement learning.",
          "Identify various applications of machine learning."
        ],
        "estimated_hours": 7.0,
        "deadline": "2024-10-27 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      },
      {
        "week_number": 2,
        "title": "Linear Regression and Data Preprocessing",
        "description": "Learning about linear regression and essential data preprocessing techniques.",
        "resources": [
          {
            "title": "Lec-3: Introduction to Regression with Real Life Examples",
            "description": "Regression is a powerful statistical technique used to predict continuous outcomes by identifying the relationship between ...",
            "url": "https://www.youtube.com/watch?v=cHT-qLnRm0E",
            "resource_type": "youtube_video",
            "duration": "PT7M19S",
            "difficulty": null,
            "tags": [
              "regression",
              "regression analysis",
              "simple linear regression",
              "machine learning",
              "what is regression",
              "linear regression analysis",
              "what is regression testing",
              "linear regression machine learning",
              "linear regression machine learning python",
              "gate smashers",
              "gate smashers ai",
              "artificial intelligence",
              "ai vs machine learning",
              "data science",
              "data analyst",
              "data scientist",
              "AI engineer",
              "gate da exam",
              "supervised learning",
              "classification",
              "Regression Basics",
              "Regression Techniques",
              "CSE Lecture"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Lec-4: Linear Regression\ud83d\udcc8 with Real life examples &amp; Calculations | Easiest Explanation",
            "description": "Linear Regression is one of the simplest and most widely used algorithms for predicting continuous outcomes. It establishes a ...",
            "url": "https://www.youtube.com/watch?v=zUQr6HAAKp4",
            "resource_type": "youtube_video",
            "duration": "PT11M1S",
            "difficulty": null,
            "tags": [
              "regression",
              "linear regression",
              "simple linear regression",
              "machine learning",
              "linear regression analysis",
              "what is regression testing",
              "linear regression machine learning python",
              "gate smashers",
              "gate smashers ai",
              "artificial intelligence",
              "ai vs machine learning",
              "how ai works",
              "how machine learning works",
              "prediction using machine learning",
              "predict using linear regression",
              "data science",
              "data analyst",
              "data scientist",
              "ai engineer",
              "gate da exam",
              "supervised learning",
              "classification"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Linear Regression Explained in Hindi ll Machine Learning Course",
            "description": "LIVE ULTIMATE DATA BOOTCAMP   https://www.5minutesengineering.com/ Myself Shridhar Mankar an Engineer l YouTuber l ...",
            "url": "https://www.youtube.com/watch?v=lzGKRSvs5HM",
            "resource_type": "youtube_video",
            "duration": "PT14M20S",
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "Fifa-World-Cup-Predictions",
            "description": "A data science project predicting FIFA World Cup outcomes using a Kaggle dataset and machine learning techniques like linear and logistic regression. The project involves data preprocessing, feature engineering, and model evaluation to analyze team performance and predict match results.",
            "url": "https://github.com/MMahad3/Fifa-World-Cup-Predictions",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "House-Price-Prediction-Project",
            "description": "This project focuses on building a machine learning model to predict house prices using various features such as square footage, number of bedrooms, and location. It demonstrates the use of Linear Regression, data preprocessing, and evaluation using Python, Pandas, and Scikit-learn.",
            "url": "https://github.com/peteleba/House-Price-Prediction-Project",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Understand the concept of linear regression.",
          "Perform data cleaning and feature scaling.",
          "Implement linear regression using Python libraries (scikit-learn)."
        ],
        "estimated_hours": 8.0,
        "deadline": "2024-11-03 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      },
      {
        "week_number": 3,
        "title": "Logistic Regression and Classification",
        "description": "Learning about logistic regression and its application in classification tasks.",
        "resources": [
          {
            "title": "Lec-5: Logistic Regression with Simplest &amp; Easiest Example | Machine Learning",
            "description": "Logistic Regression is a powerful algorithm used for binary classification tasks, where the goal is to predict one of two possible ...",
            "url": "https://www.youtube.com/watch?v=r8OjlgWpAI0",
            "resource_type": "youtube_video",
            "duration": "PT10M1S",
            "difficulty": null,
            "tags": [
              "artificial intelligence",
              "data analytics",
              "Naive Bayes",
              "Naive Bayes Classification",
              "Machine Learning",
              "machine learning",
              "data science",
              "data analyst",
              "data scientist",
              "ai engineer",
              "gate da exam",
              "supervised learning",
              "classification",
              "regression",
              "Logistic Regression",
              "Simple Logistic Regression",
              "Easy Logistic Regression",
              "Logistic Regression Example",
              "Logistic Regression for Beginners",
              "Simplest Logistic Regression Explanation",
              "Logistic Regression Explained",
              "Logistic Regression Tutorial"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Machine Learning Tutorial (Bangla) | Logistic Regression Classification Explained with Example",
            "description": "List of Data Science & AI Courses: https://aiquest.org \u2713Data Science & ML with Python Course Module: ...",
            "url": "https://www.youtube.com/watch?v=GTKBkaEsLCg",
            "resource_type": "youtube_video",
            "duration": "PT14M50S",
            "difficulty": null,
            "tags": [
              "logistic regression",
              "ml bangla",
              "machine learning bangla",
              "machine learning",
              "machine learning tutorial",
              "logistic",
              "regression",
              "classification",
              "logistic regression bangla tutorial",
              "python bangla",
              "python full course",
              "python",
              "binary logistic regression",
              "logistic regression example",
              "logistic regression machine learning"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Machine Learning Tutorial Python - 8:  Logistic Regression (Binary Classification)",
            "description": "Logistic regression is used for classification problems in machine learning. This tutorial will show you how to use sklearn ...",
            "url": "https://www.youtube.com/watch?v=zM4VZR0px8E",
            "resource_type": "youtube_video",
            "duration": "PT19M19S",
            "difficulty": null,
            "tags": [
              "logistic regression python tutorial",
              "sklearn logistic regression tutorial",
              "logistic regression machine learning",
              "logistic regression example",
              "logistic regression python",
              "logistic regression in python",
              "binary classification",
              "python logistic regression",
              "logistic regression in machine learning",
              "logistic regression sklearn",
              "logistic regression",
              "binary logistic regression",
              "logistic regression model",
              "logistic regression algorithm",
              "how logistic regression work"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "all-classification-templetes-for-ML",
            "description": "Classification - Machine Learning This is \u2018Classification\u2019 tutorial which is a part of the Machine Learning course offered by Simplilearn. We will learn Classification algorithms, types of classification algorithms, support vector machines(SVM), Naive Bayes, Decision Tree and Random Forest Classifier in this tutorial.  Objectives Let us look at some of the objectives covered under this section of Machine Learning tutorial.  Define Classification and list its algorithms Describe Logistic Regression and Sigmoid Probability Explain K-Nearest Neighbors and KNN classification Understand Support Vector Machines, Polynomial Kernel, and Kernel Trick Analyze Kernel Support Vector Machines with an example Implement the Na\u00efve Bayes Classifier Demonstrate Decision Tree Classifier Describe Random Forest Classifier Classification: Meaning Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. It predicts a class for an input variable as well.  There are 2 types of Classification:   Binomial Multi-Class Classification: Use Cases Some of the key areas where classification cases are being used:  To find whether an email received is a spam or ham To identify customer segments To find if a bank loan is granted To identify if a kid will pass or fail in an examination Classification: Example Social media sentiment analysis has two potential outcomes, positive or negative, as displayed by the chart given below.  https://www.simplilearn.com/ice9/free_resources_article_thumb/classification-example-machine-learning.JPG  This chart shows the classification of the Iris flower dataset into its three sub-species indicated by codes 0, 1, and 2. https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-flower-dataset-graph.JPG  The test set dots represent the assignment of new test data points to one class or the other based on the trained classifier model. Types of Classification Algorithms Let\u2019s have a quick look into the types of Classification Algorithm below.  Linear Models Logistic Regression Support Vector Machines Nonlinear models K-nearest Neighbors (KNN) Kernel Support Vector Machines (SVM) Na\u00efve Bayes Decision Tree Classification Random Forest Classification Logistic Regression: Meaning Let us understand the Logistic Regression model below.  This refers to a regression model that is used for classification. This method is widely used for binary classification problems. It can also be extended to multi-class classification problems. Here, the dependent variable is categorical: y \u03f5 {0, 1} A binary dependent variable can have only two values, like 0 or 1, win or lose, pass or fail, healthy or sick, etc In this case, you model the probability distribution of output y as 1 or 0. This is called the sigmoid probability (\u03c3). If \u03c3(\u03b8 Tx) > 0.5, set y = 1, else set y = 0 Unlike Linear Regression (and its Normal Equation solution), there is no closed form solution for finding optimal weights of Logistic Regression. Instead, you must solve this with maximum likelihood estimation (a probability model to detect the maximum likelihood of something happening). It can be used to calculate the probability of a given outcome in a binary model, like the probability of being classified as sick or passing an exam. https://www.simplilearn.com/ice9/free_resources_article_thumb/logistic-regression-example-graph.JPG  Sigmoid Probability The probability in the logistic regression is often represented by the Sigmoid function (also called the logistic function or the S-curve):  https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-function-machine-learning.JPG  In this equation, t represents data values * the number of hours studied and S(t) represents the probability of passing the exam. Assume sigmoid function: https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-probability-machine-learning.JPG  g(z) tends toward 1 as z -> infinity , and g(z) tends toward 0 as z -> infinity  K-nearest Neighbors (KNN) K-nearest Neighbors algorithm is used to assign a data point to clusters based on similarity measurement. It uses a supervised method for classification.  The steps to writing a k-means algorithm are as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-distribution-graph-machine-learning.JPG  Choose the number of k and a distance metric. (k = 5 is common) Find k-nearest neighbors of the sample that you want to classify Assign the class label by majority vote. KNN Classification A new input point is classified in the category such that it has the most number of neighbors from that category. For example:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-classification-machine-learning.JPG  Classify a patient as high risk or low risk. Mark email as spam or ham. Keen on learning about Classification Algorithms in Machine Learning? Click here!  Support Vector Machine (SVM) Let us understand Support Vector Machine (SVM) in detail below.  SVMs are classification algorithms used to assign data to various classes. They involve detecting hyperplanes which segregate data into classes. SVMs are very versatile and are also capable of performing linear or nonlinear classification, regression, and outlier detection. Once ideal hyperplanes are discovered, new data points can be easily classified. https://www.simplilearn.com/ice9/free_resources_article_thumb/support-vector-machines-graph-machine-learning.JPG  The optimization objective is to find \u201cmaximum margin hyperplane\u201d that is farthest from the closest points in the two classes (these points are called support vectors). In the given figure, the middle line represents the hyperplane. SVM Example Let\u2019s look at this image below and have an idea about SVM in general.  Hyperplanes with larger margins have lower generalization error. The positive and negative hyperplanes are represented by: https://www.simplilearn.com/ice9/free_resources_article_thumb/positive-negative-hyperplanes-machine-learning.JPG  Classification of any new input sample xtest :  If w0 + wTxtest > 1, the sample xtest is said to be in the class toward the right of the positive hyperplane. If w0 + wTxtest < -1, the sample xtest is said to be in the class toward the left of the negative hyperplane. When you subtract the two equations, you get:  https://www.simplilearn.com/ice9/free_resources_article_thumb/equation-subtraction-machine-learning.JPG  Length of vector w is (L2 norm length):  https://www.simplilearn.com/ice9/free_resources_article_thumb/length-of-vector-machine-learning.JPG  You normalize with the length of w to arrive at:  https://www.simplilearn.com/ice9/free_resources_article_thumb/normalize-equation-machine-learning.JPG  SVM: Hard Margin Classification Given below are some points to understand Hard Margin Classification.  The left side of equation SVM-1 given above can be interpreted as the distance between the positive (+ve) and negative (-ve) hyperplanes; in other words, it is the margin that can be maximized. Hence the objective of the function is to maximize with the constraint that the samples are classified correctly, which is represented as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-machine-learning.JPG  This means that you are minimizing \u2016w\u2016. This also means that all positive samples are on one side of the positive hyperplane and all negative samples are on the other side of the negative hyperplane. This can be written concisely as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-formula.JPG  Minimizing \u2016w\u2016 is the same as minimizing. This figure is better as it is differentiable even at w = 0. The approach listed above is called \u201chard margin linear SVM classifier.\u201d SVM: Soft Margin Classification   Given below are some points to understand Soft Margin Classification.  To allow for linear constraints to be relaxed for nonlinearly separable data, a slack variable is introduced. (i) measures how much ith instance is allowed to violate the margin. The slack variable is simply added to the linear constraints. https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-machine-learning.JPG  Subject to the above constraints, the new objective to be minimized becomes: https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-formula.JPG  You have two conflicting objectives now\u2014minimizing slack variable to reduce margin violations and minimizing to increase the margin. The hyperparameter C allows us to define this trade-off. Large values of C correspond to larger error penalties (so smaller margins), whereas smaller values of C allow for higher misclassification errors and larger margins. https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-learning-certification-video-preview.jpg  SVM: Regularization The concept of C is the reverse of regularization. Higher C means lower regularization, which increases bias and lowers the variance (causing overfitting).  https://www.simplilearn.com/ice9/free_resources_article_thumb/concept-of-c-graph-machine-learning.JPG  IRIS Data Set The Iris dataset contains measurements of 150 IRIS flowers from three different species:  Setosa Versicolor Viriginica Each row represents one sample. Flower measurements in centimeters are stored as columns. These are called features.  IRIS Data Set: SVM Let\u2019s train an SVM model using sci-kit-learn for the Iris dataset:  https://www.simplilearn.com/ice9/free_resources_article_thumb/svm-model-graph-machine-learning.JPG  Nonlinear SVM Classification There are two ways to solve nonlinear SVMs:  by adding polynomial features by adding similarity features Polynomial features can be added to datasets; in some cases, this can create a linearly separable dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/nonlinear-classification-svm-machine-learning.JPG  In the figure on the left, there is only 1 feature x1. This dataset is not linearly separable. If you add x2 = (x1)2 (figure on the right), the data becomes linearly separable. Polynomial Kernel In sci-kit-learn, one can use a Pipeline class for creating polynomial features. Classification results for the Moons dataset are shown in the figure.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-machine-learning.JPG  Polynomial Kernel with Kernel Trick Let us look at the image below and understand Kernel Trick in detail.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-with-kernel-trick.JPG  For large dimensional datasets, adding too many polynomial features can slow down the model. You can apply a kernel trick with the effect of polynomial features without actually adding them. The code is shown (SVC class) below trains an SVM classifier using a 3rd-degree polynomial kernel but with a kernel trick. https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-equation-machine-learning.JPG  The hyperparameter coef\u03b8 controls the influence of high-degree polynomials.  Kernel SVM Let us understand in detail about Kernel SVM.  Kernel SVMs are used for classification of nonlinear data. In the chart, nonlinear data is projected into a higher dimensional space via a mapping function where it becomes linearly separable. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-machine-learning.JPG  In the higher dimension, a linear separating hyperplane can be derived and used for classification.  A reverse projection of the higher dimension back to original feature space takes it back to nonlinear shape.  As mentioned previously, SVMs can be kernelized to solve nonlinear classification problems. You can create a sample dataset for XOR gate (nonlinear problem) from NumPy. 100 samples will be assigned the class sample 1, and 100 samples will be assigned the class label -1. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-graph-machine-learning.JPG  As you can see, this data is not linearly separable.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-non-separable.JPG  You now use the kernel trick to classify XOR dataset created earlier.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-xor-machine-learning.JPG  Na\u00efve Bayes Classifier What is Naive Bayes Classifier?  Have you ever wondered how your mail provider implements spam filtering or how online news channels perform news text classification or even how companies perform sentiment analysis of their audience on social media? All of this and more are done through a machine learning algorithm called Naive Bayes Classifier.  Naive Bayes Named after Thomas Bayes from the 1700s who first coined this in the Western literature. Naive Bayes classifier works on the principle of conditional probability as given by the Bayes theorem.  Advantages of Naive Bayes Classifier Listed below are six benefits of Naive Bayes Classifier.  Very simple and easy to implement Needs less training data Handles both continuous and discrete data Highly scalable with the number of predictors and data points As it is fast, it can be used in real-time predictions Not sensitive to irrelevant features Bayes Theorem We will understand Bayes Theorem in detail from the points mentioned below.  According to the Bayes model, the conditional probability P(Y|X) can be calculated as:  P(Y|X) = P(X|Y)P(Y) / P(X)  This means you have to estimate a very large number of P(X|Y) probabilities for a relatively small vector space X. For example, for a Boolean Y and 30 possible Boolean attributes in the X vector, you will have to estimate 3 billion probabilities P(X|Y). To make it practical, a Na\u00efve Bayes classifier is used, which assumes conditional independence of P(X) to each other, with a given value of Y. This reduces the number of probability estimates to 2*30=60 in the above example. Na\u00efve Bayes Classifier for SMS Spam Detection Consider a labeled SMS database having 5574 messages. It has messages as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-machine-learning.JPG  Each message is marked as spam or ham in the data set. Let\u2019s train a model with Na\u00efve Bayes algorithm to detect spam from ham. The message lengths and their frequency (in the training dataset) are as shown below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-spam-detection.JPG  Analyze the logic you use to train an algorithm to detect spam:  Split each message into individual words/tokens (bag of words). Lemmatize the data (each word takes its base form, like \u201cwalking\u201d or \u201cwalked\u201d is replaced with \u201cwalk\u201d). Convert data to vectors using scikit-learn module CountVectorizer. Run TFIDF to remove common words like \u201cis,\u201d \u201care,\u201d \u201cand.\u201d Now apply scikit-learn module for Na\u00efve Bayes MultinomialNB to get the Spam Detector. This spam detector can then be used to classify a random new message as spam or ham. Next, the accuracy of the spam detector is checked using the Confusion Matrix. For the SMS spam example above, the confusion matrix is shown on the right. Accuracy Rate = Correct / Total = (4827 + 592)/5574 = 97.21% Error Rate = Wrong / Total = (155 + 0)/5574 = 2.78% https://www.simplilearn.com/ice9/free_resources_article_thumb/confusion-matrix-machine-learning.JPG  Although confusion Matrix is useful, some more precise metrics are provided by Precision and Recall.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-recall-matrix-machine-learning.JPG  Precision refers to the accuracy of positive predictions.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-formula-machine-learning.JPG  Recall refers to the ratio of positive instances that are correctly detected by the classifier (also known as True positive rate or TPR).  https://www.simplilearn.com/ice9/free_resources_article_thumb/recall-formula-machine-learning.JPG  Precision/Recall Trade-off To detect age-appropriate videos for kids, you need high precision (low recall) to ensure that only safe videos make the cut (even though a few safe videos may be left out).  The high recall is needed (low precision is acceptable) in-store surveillance to catch shoplifters; a few false alarms are acceptable, but all shoplifters must be caught.  Learn about Naive Bayes in detail. Click here!  Decision Tree Classifier Some aspects of the Decision Tree Classifier mentioned below are.  Decision Trees (DT) can be used both for classification and regression. The advantage of decision trees is that they require very little data preparation. They do not require feature scaling or centering at all. They are also the fundamental components of Random Forests, one of the most powerful ML algorithms. Unlike Random Forests and Neural Networks (which do black-box modeling), Decision Trees are white box models, which means that inner workings of these models are clearly understood. In the case of classification, the data is segregated based on a series of questions. Any new data point is assigned to the selected leaf node. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-machine-learning.JPG  Start at the tree root and split the data on the feature using the decision algorithm, resulting in the largest information gain (IG). This splitting procedure is then repeated in an iterative process at each child node until the leaves are pure. This means that the samples at each node belonging to the same class. In practice, you can set a limit on the depth of the tree to prevent overfitting. The purity is compromised here as the final leaves may still have some impurity. The figure shows the classification of the Iris dataset. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-graph.JPG  IRIS Decision Tree Let\u2019s build a Decision Tree using scikit-learn for the Iris flower dataset and also visualize it using export_graphviz API.  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-machine-learning.JPG  The output of export_graphviz can be converted into png format:  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-output.JPG  Sample attribute stands for the number of training instances the node applies to. Value attribute stands for the number of training instances of each class the node applies to. Gini impurity measures the node\u2019s impurity. A node is \u201cpure\u201d (gini=0) if all training instances it applies to belong to the same class. https://www.simplilearn.com/ice9/free_resources_article_thumb/impurity-formula-machine-learning.JPG  For example, for Versicolor (green color node), the Gini is 1-(0/54)2 -(49/54)2 -(5/54) 2 \u2248 0.168  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-sample.JPG  Decision Boundaries Let us learn to create decision boundaries below.  For the first node (depth 0), the solid line splits the data (Iris-Setosa on left). Gini is 0 for Setosa node, so no further split is possible. The second node (depth 1) splits the data into Versicolor and Virginica. If max_depth were set as 3, a third split would happen (vertical dotted line). https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-boundaries.JPG  For a sample with petal length 5 cm and petal width 1.5 cm, the tree traverses to depth 2 left node, so the probability predictions for this sample are 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for Iris-Virginica (5/54) CART Training Algorithm Scikit-learn uses Classification and Regression Trees (CART) algorithm to train Decision Trees. CART algorithm:  Split the data into two subsets using a single feature k and threshold tk (example, petal length < \u201c2.45 cm\u201d). This is done recursively for each node. k and tk are chosen such that they produce the purest subsets (weighted by their size). The objective is to minimize the cost function as given below: https://www.simplilearn.com/ice9/free_resources_article_thumb/cart-training-algorithm-machine-learning.JPG  The algorithm stops executing if one of the following situations occurs: max_depth is reached No further splits are found for each node Other hyperparameters may be used to stop the tree: min_samples_split min_samples_leaf min_weight_fraction_leaf max_leaf_nodes Gini Impurity or Entropy Entropy is one more measure of impurity and can be used in place of Gini.  https://www.simplilearn.com/ice9/free_resources_article_thumb/gini-impurity-entrophy.JPG  It is a degree of uncertainty, and Information Gain is the reduction that occurs in entropy as one traverses down the tree. Entropy is zero for a DT node when the node contains instances of only one class. Entropy for depth 2 left node in the example given above is:  https://www.simplilearn.com/ice9/free_resources_article_thumb/entrophy-for-depth-2.JPG  Gini and Entropy both lead to similar trees.  DT: Regularization The following figure shows two decision trees on the moons dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/dt-regularization-machine-learning.JPG  The decision tree on the right is restricted by min_samples_leaf = 4. The model on the left is overfitting, while the model on the right generalizes better. Random Forest Classifier Let us have an understanding of Random Forest Classifier below.  A random forest can be considered an ensemble of decision trees (Ensemble learning). Random Forest algorithm: Draw a random bootstrap sample of size n (randomly choose n samples from the training set). Grow a decision tree from the bootstrap sample. At each node, randomly select d features. Split the node using the feature that provides the best split according to the objective function, for instance by maximizing the information gain. Repeat the steps 1 to 2 k times. (k is the number of trees you want to create, using a subset of samples) Aggregate the prediction by each tree for a new data point to assign the class label by majority vote (pick the group selected by the most number of trees and assign new data point to that group). Random Forests are opaque, which means it is difficult to visualize their inner workings. https://www.simplilearn.com/ice9/free_resources_article_thumb/random-forest-classifier-graph.JPG  However, the advantages outweigh their limitations since you do not have to worry about hyperparameters except k, which stands for the number of decision trees to be created from a subset of samples. RF is quite robust to noise from the individual decision trees. Hence, you need not prune individual decision trees. The larger the number of decision trees, the more accurate the Random Forest prediction is. (This, however, comes with higher computation cost). Key Takeaways Let us quickly run through what we have learned so far in this Classification tutorial.  Classification algorithms are supervised learning methods to split data into classes. They can work on Linear Data as well as Nonlinear Data. Logistic Regression can classify data based on weighted parameters and sigmoid conversion to calculate the probability of classes. K-nearest Neighbors (KNN) algorithm uses similar features to classify data. Support Vector Machines (SVMs) classify data by detecting the maximum margin hyperplane between data classes. Na\u00efve Bayes, a simplified Bayes Model, can help classify data using conditional probability models. Decision Trees are powerful classifiers and use tree splitting logic until pure or somewhat pure leaf node classes are attained. Random Forests apply Ensemble Learning to Decision Trees for more accurate classification predictions. Conclusion This completes \u2018Classification\u2019 tutorial. In the next tutorial, we will learn 'Unsupervised Learning with Clustering.'",
            "url": "https://github.com/sayantann11/all-classification-templetes-for-ML",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "Parkinson-Disease-Prediction",
            "description": "Introduction  Parkinson\u2019s Disease is the second most prevalent neurodegenerative disorder after Alzheimer\u2019s, affecting more than 10 million people worldwide. Parkinson\u2019s is characterized primarily by the deterioration of motor and cognitive ability. There is no single test which can be administered for diagnosis. Instead, doctors must perform a careful clinical analysis of the patient\u2019s medical history. Unfortunately, this method of diagnosis is highly inaccurate. A study from the National Institute of Neurological Disorders finds that early diagnosis (having symptoms for 5 years or less) is only 53% accurate. This is not much better than random guessing, but an early diagnosis is critical to effective treatment. Because of these difficulties, I investigate a machine learning approach to accurately diagnose Parkinson\u2019s, using a dataset of various speech features (a non-invasive yet characteristic tool) from the University of Oxford. Why speech features? Speech is very predictive and characteristic of Parkinson\u2019s disease; almost every Parkinson\u2019s patient experiences severe vocal degradation (inability to produce sustained phonations, tremor, hoarseness), so it makes sense to use voice to diagnose the disease. Voice analysis gives the added benefit of being non-invasive, inexpensive, and very easy to extract clinically. Background  Parkinson's Disease  Parkinson\u2019s is a progressive neurodegenerative condition resulting from the death of the dopamine containing cells of the substantia nigra (which plays an important role in movement). Symptoms include: \u201cfrozen\u201d facial features, bradykinesia (slowness of movement), akinesia (impairment of voluntary movement), tremor, and voice impairment. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. Performance Metrics  TP = true positive, FP = false positive, TN = true negative, FN = false negative Accuracy: (TP+TN)/(P+N) Matthews Correlation Coefficient: 1=perfect, 0=random, -1=completely inaccurate Algorithms Employed  Logistic Regression (LR): Uses the sigmoid logistic equation with weights (coefficient values) and biases (constants) to model the probability of a certain class for binary classification. An output of 1 represents one class, and an output of 0 represents the other. Training the model will learn the optimal weights and biases. Linear Discriminant Analysis (LDA): Assumes that the data is Gaussian and each feature has the same variance. LDA estimates the mean and variance for each class from the training data, and then uses properties of statistics (Bayes theorem , Gaussian distribution, etc) to compute the probability of a particular instance belonging to a given class. The class with the largest probability is the prediction. k Nearest Neighbors (KNN): Makes predictions about the validation set using the entire training set. KNN makes a prediction about a new instance by searching through the entire set to find the k \u201cclosest\u201d instances. \u201cCloseness\u201d is determined using a proximity measurement (Euclidean) across all features. The class that the majority of the k closest instances belong to is the class that the model predicts the new instance to be. Decision Tree (DT): Represented by a binary tree, where each root node represents an input variable and a split point, and each leaf node contains an output used to make a prediction. Neural Network (NN): Models the way the human brain makes decisions. Each neuron takes in 1+ inputs, and then uses an activation function to process the input with weights and biases to produce an output. Neurons can be arranged into layers, and multiple layers can form a network to model complex decisions. Training the network involves using the training instances to optimize the weights and biases. Naive Bayes (NB): Simplifies the calculation of probabilities by assuming that all features are independent of one another (a strong but effective assumption). Employs Bayes Theorem to calculate the probabilities that the instance to be predicted is in each class, then finds the class with the highest probability. Gradient Boost (GB): Generally used when seeking a model with very high predictive performance. Used to reduce bias and variance (\u201cerror\u201d) by combining multiple \u201cweak learners\u201d (not very good models) to create a \u201cstrong learner\u201d (high performance model). Involves 3 elements: a loss function (error function) to be optimized, a weak learner (decision tree) to make predictions, and an additive model to add trees to minimize the loss function. Gradient descent is used to minimize error after adding each tree (one by one). Engineering Goal  Produce a machine learning model to diagnose Parkinson\u2019s disease given various features of a patient\u2019s speech with at least 90% accuracy and/or a Matthews Correlation Coefficient of at least 0.9. Compare various algorithms and parameters to determine the best model for predicting Parkinson\u2019s.  Dataset Description  Source: the University of Oxford 195 instances (147 subjects with Parkinson\u2019s, 48 without Parkinson\u2019s) 22 features (elements that are possibly characteristic of Parkinson\u2019s, such as frequency, pitch, amplitude / period of the sound wave) 1 label (1 for Parkinson\u2019s, 0 for no Parkinson\u2019s) Project Pipeline  pipeline  Summary of Procedure  Split the Oxford Parkinson\u2019s Dataset into two parts: one for training, one for validation (evaluate how well the model performs) Train each of the following algorithms with the training set: Logistic Regression, Linear Discriminant Analysis, k Nearest Neighbors, Decision Tree, Neural Network, Naive Bayes, Gradient Boost Evaluate results using the validation set Repeat for the following training set to validation set splits: 80% training / 20% validation, 75% / 25%, and 70% / 30% Repeat for a rescaled version of the dataset (scale all the numbers in the dataset to a range from 0 to 1: this helps to reduce the effect of outliers) Conduct 5 trials and average the results Data  a_o  a_r  m_o  m_r  Data Analysis  In general, the models tended to perform the best (both in terms of accuracy and Matthews Correlation Coefficient) on the rescaled dataset with a 75-25 train-test split. The two highest performing algorithms, k Nearest Neighbors and the Neural Network, both achieved an accuracy of 98%. The NN achieved a MCC of 0.96, while KNN achieved a MCC of 0.94. These figures outperform most existing literature and significantly outperform current methods of diagnosis. Conclusion and Significance  These robust results suggest that a machine learning approach can indeed be implemented to significantly improve diagnosis methods of Parkinson\u2019s disease. Given the necessity of early diagnosis for effective treatment, my machine learning models provide a very promising alternative to the current, rather ineffective method of diagnosis. Current methods of early diagnosis are only 53% accurate, while my machine learning model produces 98% accuracy. This 45% increase is critical because an accurate, early diagnosis is needed to effectively treat the disease. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. With an earlier diagnosis, much of this degradation could have been slowed or treated. My results are very significant because Parkinson\u2019s affects over 10 million people worldwide who could benefit greatly from an early, accurate diagnosis. Not only is my machine learning approach more accurate in terms of diagnostic accuracy, it is also more scalable, less expensive, and therefore more accessible to people who might not have access to established medical facilities and professionals. The diagnosis is also much simpler, requiring only a 10-15 second voice recording and producing an immediate diagnosis. Future Research  Given more time and resources, I would investigate the following: Create a mobile application which would allow the user to record his/her voice, extract the necessary vocal features, and feed it into my machine learning model to diagnose Parkinson\u2019s. Use larger datasets in conjunction with the University of Oxford dataset. Tune and improve my models even further to achieve even better results. Investigate different structures and types of neural networks. Construct a novel algorithm specifically suited for the prediction of Parkinson\u2019s. Generalize my findings and algorithms for all types of dementia disorders, such as Alzheimer\u2019s. References  Bind, Shubham. \"A Survey of Machine Learning Based Approaches for Parkinson Disease Prediction.\" International Journal of Computer Science and Information Technologies 6 (2015): n. pag. International Journal of Computer Science and Information Technologies. 2015. Web. 8 Mar. 2017. Brooks, Megan. \"Diagnosing Parkinson's Disease Still Challenging.\" Medscape Medical News. National Institute of Neurological Disorders, 31 July 2014. Web. 20 Mar. 2017. Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) Hashmi, Sumaiya F. \"A Machine Learning Approach to Diagnosis of Parkinson\u2019s Disease.\"Claremont Colleges Scholarship. Claremont College, 2013. Web. 10 Mar. 2017. Karplus, Abraham. \"Machine Learning Algorithms for Cancer Diagnosis.\" Machine Learning Algorithms for Cancer Diagnosis (n.d.): n. pag. Mar. 2012. Web. 20 Mar. 2017. Little, Max. \"Parkinsons Data Set.\" UCI Machine Learning Repository. University of Oxford, 26 June 2008. Web. 20 Feb. 2017. Ozcift, Akin, and Arif Gulten. \"Classifier Ensemble Construction with Rotation Forest to Improve Medical Diagnosis Performance of Machine Learning Algorithms.\" Computer Methods and Programs in Biomedicine 104.3 (2011): 443-51. Semantic Scholar. 2011. Web. 15 Mar. 2017. \"Parkinson\u2019s Disease Dementia.\" UCI MIND. N.p., 19 Oct. 2015. Web. 17 Feb. 2017. Salvatore, C., A. Cerasa, I. Castiglioni, F. Gallivanone, A. Augimeri, M. Lopez, G. Arabia, M. Morelli, M.c. Gilardi, and A. Quattrone. \"Machine Learning on Brain MRI Data for Differential Diagnosis of Parkinson's Disease and Progressive Supranuclear Palsy.\"Journal of Neuroscience Methods 222 (2014): 230-37. 2014. Web. 18 Mar. 2017. Shahbakhi, Mohammad, Danial Taheri Far, and Ehsan Tahami. \"Speech Analysis for Diagnosis of Parkinson\u2019s Disease Using Genetic Algorithm and Support Vector Machine.\"Journal of Biomedical Science and Engineering 07.04 (2014): 147-56. Scientific Research. July 2014. Web. 2 Mar. 2017. \"Speech and Communication.\" Speech and Communication. Parkinson's Disease Foundation, n.d. Web. 22 Mar. 2017. Sriram, Tarigoppula V. S., M. Venkateswara Rao, G. V. Satya Narayana, and D. S. V. G. K. Kaladhar. \"Diagnosis of Parkinson Disease Using Machine Learning and Data Mining Systems from Voice Dataset.\" SpringerLink. Springer, Cham, 01 Jan. 1970. Web. 17 Mar. 2017.",
            "url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Understand the concept of logistic regression.",
          "Implement logistic regression using scikit-learn.",
          "Evaluate the performance of a classification model."
        ],
        "estimated_hours": 10.0,
        "deadline": "2024-11-10 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      },
      {
        "week_number": 4,
        "title": "Decision Trees and Ensemble Methods",
        "description": "Exploring decision trees and ensemble methods like Random Forest and Gradient Boosting.",
        "resources": [
          {
            "title": "Lec-12: Introduction to Ensemble Learning with Real Life Examples | Machine\u2699\ufe0f Learning",
            "description": "Ensemble Learning is a powerful machine learning technique that combines multiple models to boost accuracy and performance.",
            "url": "https://www.youtube.com/watch?v=qQjOWmf8I_I",
            "resource_type": "youtube_video",
            "duration": "PT5M58S",
            "difficulty": null,
            "tags": [
              "Ensemble Learning",
              "machine learning",
              "ensemble learning in machine learning",
              "ensemble learning python",
              "ensemble learning tutorial",
              "ensemble learning",
              "machine learning tutorial",
              "what is machine learning",
              "machine learning tutorial for beginners",
              "machine learning models for beginners",
              "what is ensemble learning?",
              "types of ensemble learning",
              "applications of ensemble learning",
              "Ensemble Learning Techniques",
              "ensemble learning techniques",
              "artificial intelligence"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Lec-9: Introduction to Decision Tree \ud83c\udf32 with Real life examples",
            "description": "Decision Trees are among the most widely used algorithms in machine learning, ideal for both classification and regression tasks.",
            "url": "https://www.youtube.com/watch?v=mvveVcbHynE",
            "resource_type": "youtube_video",
            "duration": "PT6M7S",
            "difficulty": null,
            "tags": [
              "Decision tree",
              "machine learning",
              "data science",
              "data analyst",
              "data scientist",
              "ai engineer",
              "gate da exam",
              "supervised learning",
              "classification",
              "regression",
              "Introduction to Decision Tree",
              "Decision Trees in CSE",
              "Data Science Tutorials",
              "Machine Learning Basics",
              "Classification Algorithms",
              "Decision Tree Algorithm",
              "Artificial Intelligence",
              "Decision Tree Explained",
              "Decision Tree Visualization",
              "Decision Tree Example",
              "Decision Tree Tutorial for Beginners"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Examples of Decision Tree | Machine Learning | Tutorial for Beginners | Great Learning",
            "description": "Algorithms-Decision tree ID3, CART, C5.0, C4.5 -Decision tree is a classification technique, consists of nodes and branches.",
            "url": "https://www.youtube.com/watch?v=fH0f77s95KA",
            "resource_type": "youtube_video",
            "duration": "PT14M20S",
            "difficulty": null,
            "tags": [
              "Great Learning",
              "Great Lakes",
              "Machine learning",
              "Artificial Intelligence",
              "decision tree",
              "decision tree in machine learning",
              "decision tree analysis",
              "introduction to decision tree",
              "tutorial",
              "tutorial for beginners",
              "decision tree explained",
              "decision tree data mining",
              "decision tree tutorial",
              "decision tree tutorial machine learning",
              "learn decision trees",
              "data mining",
              "data analytics",
              "decision tree example",
              "decision tree algorithm",
              "decision tree example with solution"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "all-classification-templetes-for-ML",
            "description": "Classification - Machine Learning This is \u2018Classification\u2019 tutorial which is a part of the Machine Learning course offered by Simplilearn. We will learn Classification algorithms, types of classification algorithms, support vector machines(SVM), Naive Bayes, Decision Tree and Random Forest Classifier in this tutorial.  Objectives Let us look at some of the objectives covered under this section of Machine Learning tutorial.  Define Classification and list its algorithms Describe Logistic Regression and Sigmoid Probability Explain K-Nearest Neighbors and KNN classification Understand Support Vector Machines, Polynomial Kernel, and Kernel Trick Analyze Kernel Support Vector Machines with an example Implement the Na\u00efve Bayes Classifier Demonstrate Decision Tree Classifier Describe Random Forest Classifier Classification: Meaning Classification is a type of supervised learning. It specifies the class to which data elements belong to and is best used when the output has finite and discrete values. It predicts a class for an input variable as well.  There are 2 types of Classification:   Binomial Multi-Class Classification: Use Cases Some of the key areas where classification cases are being used:  To find whether an email received is a spam or ham To identify customer segments To find if a bank loan is granted To identify if a kid will pass or fail in an examination Classification: Example Social media sentiment analysis has two potential outcomes, positive or negative, as displayed by the chart given below.  https://www.simplilearn.com/ice9/free_resources_article_thumb/classification-example-machine-learning.JPG  This chart shows the classification of the Iris flower dataset into its three sub-species indicated by codes 0, 1, and 2. https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-flower-dataset-graph.JPG  The test set dots represent the assignment of new test data points to one class or the other based on the trained classifier model. Types of Classification Algorithms Let\u2019s have a quick look into the types of Classification Algorithm below.  Linear Models Logistic Regression Support Vector Machines Nonlinear models K-nearest Neighbors (KNN) Kernel Support Vector Machines (SVM) Na\u00efve Bayes Decision Tree Classification Random Forest Classification Logistic Regression: Meaning Let us understand the Logistic Regression model below.  This refers to a regression model that is used for classification. This method is widely used for binary classification problems. It can also be extended to multi-class classification problems. Here, the dependent variable is categorical: y \u03f5 {0, 1} A binary dependent variable can have only two values, like 0 or 1, win or lose, pass or fail, healthy or sick, etc In this case, you model the probability distribution of output y as 1 or 0. This is called the sigmoid probability (\u03c3). If \u03c3(\u03b8 Tx) > 0.5, set y = 1, else set y = 0 Unlike Linear Regression (and its Normal Equation solution), there is no closed form solution for finding optimal weights of Logistic Regression. Instead, you must solve this with maximum likelihood estimation (a probability model to detect the maximum likelihood of something happening). It can be used to calculate the probability of a given outcome in a binary model, like the probability of being classified as sick or passing an exam. https://www.simplilearn.com/ice9/free_resources_article_thumb/logistic-regression-example-graph.JPG  Sigmoid Probability The probability in the logistic regression is often represented by the Sigmoid function (also called the logistic function or the S-curve):  https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-function-machine-learning.JPG  In this equation, t represents data values * the number of hours studied and S(t) represents the probability of passing the exam. Assume sigmoid function: https://www.simplilearn.com/ice9/free_resources_article_thumb/sigmoid-probability-machine-learning.JPG  g(z) tends toward 1 as z -> infinity , and g(z) tends toward 0 as z -> infinity  K-nearest Neighbors (KNN) K-nearest Neighbors algorithm is used to assign a data point to clusters based on similarity measurement. It uses a supervised method for classification.  The steps to writing a k-means algorithm are as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-distribution-graph-machine-learning.JPG  Choose the number of k and a distance metric. (k = 5 is common) Find k-nearest neighbors of the sample that you want to classify Assign the class label by majority vote. KNN Classification A new input point is classified in the category such that it has the most number of neighbors from that category. For example:  https://www.simplilearn.com/ice9/free_resources_article_thumb/knn-classification-machine-learning.JPG  Classify a patient as high risk or low risk. Mark email as spam or ham. Keen on learning about Classification Algorithms in Machine Learning? Click here!  Support Vector Machine (SVM) Let us understand Support Vector Machine (SVM) in detail below.  SVMs are classification algorithms used to assign data to various classes. They involve detecting hyperplanes which segregate data into classes. SVMs are very versatile and are also capable of performing linear or nonlinear classification, regression, and outlier detection. Once ideal hyperplanes are discovered, new data points can be easily classified. https://www.simplilearn.com/ice9/free_resources_article_thumb/support-vector-machines-graph-machine-learning.JPG  The optimization objective is to find \u201cmaximum margin hyperplane\u201d that is farthest from the closest points in the two classes (these points are called support vectors). In the given figure, the middle line represents the hyperplane. SVM Example Let\u2019s look at this image below and have an idea about SVM in general.  Hyperplanes with larger margins have lower generalization error. The positive and negative hyperplanes are represented by: https://www.simplilearn.com/ice9/free_resources_article_thumb/positive-negative-hyperplanes-machine-learning.JPG  Classification of any new input sample xtest :  If w0 + wTxtest > 1, the sample xtest is said to be in the class toward the right of the positive hyperplane. If w0 + wTxtest < -1, the sample xtest is said to be in the class toward the left of the negative hyperplane. When you subtract the two equations, you get:  https://www.simplilearn.com/ice9/free_resources_article_thumb/equation-subtraction-machine-learning.JPG  Length of vector w is (L2 norm length):  https://www.simplilearn.com/ice9/free_resources_article_thumb/length-of-vector-machine-learning.JPG  You normalize with the length of w to arrive at:  https://www.simplilearn.com/ice9/free_resources_article_thumb/normalize-equation-machine-learning.JPG  SVM: Hard Margin Classification Given below are some points to understand Hard Margin Classification.  The left side of equation SVM-1 given above can be interpreted as the distance between the positive (+ve) and negative (-ve) hyperplanes; in other words, it is the margin that can be maximized. Hence the objective of the function is to maximize with the constraint that the samples are classified correctly, which is represented as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-machine-learning.JPG  This means that you are minimizing \u2016w\u2016. This also means that all positive samples are on one side of the positive hyperplane and all negative samples are on the other side of the negative hyperplane. This can be written concisely as : https://www.simplilearn.com/ice9/free_resources_article_thumb/hard-margin-classification-formula.JPG  Minimizing \u2016w\u2016 is the same as minimizing. This figure is better as it is differentiable even at w = 0. The approach listed above is called \u201chard margin linear SVM classifier.\u201d SVM: Soft Margin Classification   Given below are some points to understand Soft Margin Classification.  To allow for linear constraints to be relaxed for nonlinearly separable data, a slack variable is introduced. (i) measures how much ith instance is allowed to violate the margin. The slack variable is simply added to the linear constraints. https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-machine-learning.JPG  Subject to the above constraints, the new objective to be minimized becomes: https://www.simplilearn.com/ice9/free_resources_article_thumb/soft-margin-calculation-formula.JPG  You have two conflicting objectives now\u2014minimizing slack variable to reduce margin violations and minimizing to increase the margin. The hyperparameter C allows us to define this trade-off. Large values of C correspond to larger error penalties (so smaller margins), whereas smaller values of C allow for higher misclassification errors and larger margins. https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-learning-certification-video-preview.jpg  SVM: Regularization The concept of C is the reverse of regularization. Higher C means lower regularization, which increases bias and lowers the variance (causing overfitting).  https://www.simplilearn.com/ice9/free_resources_article_thumb/concept-of-c-graph-machine-learning.JPG  IRIS Data Set The Iris dataset contains measurements of 150 IRIS flowers from three different species:  Setosa Versicolor Viriginica Each row represents one sample. Flower measurements in centimeters are stored as columns. These are called features.  IRIS Data Set: SVM Let\u2019s train an SVM model using sci-kit-learn for the Iris dataset:  https://www.simplilearn.com/ice9/free_resources_article_thumb/svm-model-graph-machine-learning.JPG  Nonlinear SVM Classification There are two ways to solve nonlinear SVMs:  by adding polynomial features by adding similarity features Polynomial features can be added to datasets; in some cases, this can create a linearly separable dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/nonlinear-classification-svm-machine-learning.JPG  In the figure on the left, there is only 1 feature x1. This dataset is not linearly separable. If you add x2 = (x1)2 (figure on the right), the data becomes linearly separable. Polynomial Kernel In sci-kit-learn, one can use a Pipeline class for creating polynomial features. Classification results for the Moons dataset are shown in the figure.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-machine-learning.JPG  Polynomial Kernel with Kernel Trick Let us look at the image below and understand Kernel Trick in detail.  https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-with-kernel-trick.JPG  For large dimensional datasets, adding too many polynomial features can slow down the model. You can apply a kernel trick with the effect of polynomial features without actually adding them. The code is shown (SVC class) below trains an SVM classifier using a 3rd-degree polynomial kernel but with a kernel trick. https://www.simplilearn.com/ice9/free_resources_article_thumb/polynomial-kernel-equation-machine-learning.JPG  The hyperparameter coef\u03b8 controls the influence of high-degree polynomials.  Kernel SVM Let us understand in detail about Kernel SVM.  Kernel SVMs are used for classification of nonlinear data. In the chart, nonlinear data is projected into a higher dimensional space via a mapping function where it becomes linearly separable. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-machine-learning.JPG  In the higher dimension, a linear separating hyperplane can be derived and used for classification.  A reverse projection of the higher dimension back to original feature space takes it back to nonlinear shape.  As mentioned previously, SVMs can be kernelized to solve nonlinear classification problems. You can create a sample dataset for XOR gate (nonlinear problem) from NumPy. 100 samples will be assigned the class sample 1, and 100 samples will be assigned the class label -1. https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-graph-machine-learning.JPG  As you can see, this data is not linearly separable.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-non-separable.JPG  You now use the kernel trick to classify XOR dataset created earlier.  https://www.simplilearn.com/ice9/free_resources_article_thumb/kernel-svm-xor-machine-learning.JPG  Na\u00efve Bayes Classifier What is Naive Bayes Classifier?  Have you ever wondered how your mail provider implements spam filtering or how online news channels perform news text classification or even how companies perform sentiment analysis of their audience on social media? All of this and more are done through a machine learning algorithm called Naive Bayes Classifier.  Naive Bayes Named after Thomas Bayes from the 1700s who first coined this in the Western literature. Naive Bayes classifier works on the principle of conditional probability as given by the Bayes theorem.  Advantages of Naive Bayes Classifier Listed below are six benefits of Naive Bayes Classifier.  Very simple and easy to implement Needs less training data Handles both continuous and discrete data Highly scalable with the number of predictors and data points As it is fast, it can be used in real-time predictions Not sensitive to irrelevant features Bayes Theorem We will understand Bayes Theorem in detail from the points mentioned below.  According to the Bayes model, the conditional probability P(Y|X) can be calculated as:  P(Y|X) = P(X|Y)P(Y) / P(X)  This means you have to estimate a very large number of P(X|Y) probabilities for a relatively small vector space X. For example, for a Boolean Y and 30 possible Boolean attributes in the X vector, you will have to estimate 3 billion probabilities P(X|Y). To make it practical, a Na\u00efve Bayes classifier is used, which assumes conditional independence of P(X) to each other, with a given value of Y. This reduces the number of probability estimates to 2*30=60 in the above example. Na\u00efve Bayes Classifier for SMS Spam Detection Consider a labeled SMS database having 5574 messages. It has messages as given below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-machine-learning.JPG  Each message is marked as spam or ham in the data set. Let\u2019s train a model with Na\u00efve Bayes algorithm to detect spam from ham. The message lengths and their frequency (in the training dataset) are as shown below:  https://www.simplilearn.com/ice9/free_resources_article_thumb/naive-bayes-spam-spam-detection.JPG  Analyze the logic you use to train an algorithm to detect spam:  Split each message into individual words/tokens (bag of words). Lemmatize the data (each word takes its base form, like \u201cwalking\u201d or \u201cwalked\u201d is replaced with \u201cwalk\u201d). Convert data to vectors using scikit-learn module CountVectorizer. Run TFIDF to remove common words like \u201cis,\u201d \u201care,\u201d \u201cand.\u201d Now apply scikit-learn module for Na\u00efve Bayes MultinomialNB to get the Spam Detector. This spam detector can then be used to classify a random new message as spam or ham. Next, the accuracy of the spam detector is checked using the Confusion Matrix. For the SMS spam example above, the confusion matrix is shown on the right. Accuracy Rate = Correct / Total = (4827 + 592)/5574 = 97.21% Error Rate = Wrong / Total = (155 + 0)/5574 = 2.78% https://www.simplilearn.com/ice9/free_resources_article_thumb/confusion-matrix-machine-learning.JPG  Although confusion Matrix is useful, some more precise metrics are provided by Precision and Recall.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-recall-matrix-machine-learning.JPG  Precision refers to the accuracy of positive predictions.  https://www.simplilearn.com/ice9/free_resources_article_thumb/precision-formula-machine-learning.JPG  Recall refers to the ratio of positive instances that are correctly detected by the classifier (also known as True positive rate or TPR).  https://www.simplilearn.com/ice9/free_resources_article_thumb/recall-formula-machine-learning.JPG  Precision/Recall Trade-off To detect age-appropriate videos for kids, you need high precision (low recall) to ensure that only safe videos make the cut (even though a few safe videos may be left out).  The high recall is needed (low precision is acceptable) in-store surveillance to catch shoplifters; a few false alarms are acceptable, but all shoplifters must be caught.  Learn about Naive Bayes in detail. Click here!  Decision Tree Classifier Some aspects of the Decision Tree Classifier mentioned below are.  Decision Trees (DT) can be used both for classification and regression. The advantage of decision trees is that they require very little data preparation. They do not require feature scaling or centering at all. They are also the fundamental components of Random Forests, one of the most powerful ML algorithms. Unlike Random Forests and Neural Networks (which do black-box modeling), Decision Trees are white box models, which means that inner workings of these models are clearly understood. In the case of classification, the data is segregated based on a series of questions. Any new data point is assigned to the selected leaf node. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-machine-learning.JPG  Start at the tree root and split the data on the feature using the decision algorithm, resulting in the largest information gain (IG). This splitting procedure is then repeated in an iterative process at each child node until the leaves are pure. This means that the samples at each node belonging to the same class. In practice, you can set a limit on the depth of the tree to prevent overfitting. The purity is compromised here as the final leaves may still have some impurity. The figure shows the classification of the Iris dataset. https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-classifier-graph.JPG  IRIS Decision Tree Let\u2019s build a Decision Tree using scikit-learn for the Iris flower dataset and also visualize it using export_graphviz API.  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-machine-learning.JPG  The output of export_graphviz can be converted into png format:  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-output.JPG  Sample attribute stands for the number of training instances the node applies to. Value attribute stands for the number of training instances of each class the node applies to. Gini impurity measures the node\u2019s impurity. A node is \u201cpure\u201d (gini=0) if all training instances it applies to belong to the same class. https://www.simplilearn.com/ice9/free_resources_article_thumb/impurity-formula-machine-learning.JPG  For example, for Versicolor (green color node), the Gini is 1-(0/54)2 -(49/54)2 -(5/54) 2 \u2248 0.168  https://www.simplilearn.com/ice9/free_resources_article_thumb/iris-decision-tree-sample.JPG  Decision Boundaries Let us learn to create decision boundaries below.  For the first node (depth 0), the solid line splits the data (Iris-Setosa on left). Gini is 0 for Setosa node, so no further split is possible. The second node (depth 1) splits the data into Versicolor and Virginica. If max_depth were set as 3, a third split would happen (vertical dotted line). https://www.simplilearn.com/ice9/free_resources_article_thumb/decision-tree-boundaries.JPG  For a sample with petal length 5 cm and petal width 1.5 cm, the tree traverses to depth 2 left node, so the probability predictions for this sample are 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for Iris-Virginica (5/54) CART Training Algorithm Scikit-learn uses Classification and Regression Trees (CART) algorithm to train Decision Trees. CART algorithm:  Split the data into two subsets using a single feature k and threshold tk (example, petal length < \u201c2.45 cm\u201d). This is done recursively for each node. k and tk are chosen such that they produce the purest subsets (weighted by their size). The objective is to minimize the cost function as given below: https://www.simplilearn.com/ice9/free_resources_article_thumb/cart-training-algorithm-machine-learning.JPG  The algorithm stops executing if one of the following situations occurs: max_depth is reached No further splits are found for each node Other hyperparameters may be used to stop the tree: min_samples_split min_samples_leaf min_weight_fraction_leaf max_leaf_nodes Gini Impurity or Entropy Entropy is one more measure of impurity and can be used in place of Gini.  https://www.simplilearn.com/ice9/free_resources_article_thumb/gini-impurity-entrophy.JPG  It is a degree of uncertainty, and Information Gain is the reduction that occurs in entropy as one traverses down the tree. Entropy is zero for a DT node when the node contains instances of only one class. Entropy for depth 2 left node in the example given above is:  https://www.simplilearn.com/ice9/free_resources_article_thumb/entrophy-for-depth-2.JPG  Gini and Entropy both lead to similar trees.  DT: Regularization The following figure shows two decision trees on the moons dataset.  https://www.simplilearn.com/ice9/free_resources_article_thumb/dt-regularization-machine-learning.JPG  The decision tree on the right is restricted by min_samples_leaf = 4. The model on the left is overfitting, while the model on the right generalizes better. Random Forest Classifier Let us have an understanding of Random Forest Classifier below.  A random forest can be considered an ensemble of decision trees (Ensemble learning). Random Forest algorithm: Draw a random bootstrap sample of size n (randomly choose n samples from the training set). Grow a decision tree from the bootstrap sample. At each node, randomly select d features. Split the node using the feature that provides the best split according to the objective function, for instance by maximizing the information gain. Repeat the steps 1 to 2 k times. (k is the number of trees you want to create, using a subset of samples) Aggregate the prediction by each tree for a new data point to assign the class label by majority vote (pick the group selected by the most number of trees and assign new data point to that group). Random Forests are opaque, which means it is difficult to visualize their inner workings. https://www.simplilearn.com/ice9/free_resources_article_thumb/random-forest-classifier-graph.JPG  However, the advantages outweigh their limitations since you do not have to worry about hyperparameters except k, which stands for the number of decision trees to be created from a subset of samples. RF is quite robust to noise from the individual decision trees. Hence, you need not prune individual decision trees. The larger the number of decision trees, the more accurate the Random Forest prediction is. (This, however, comes with higher computation cost). Key Takeaways Let us quickly run through what we have learned so far in this Classification tutorial.  Classification algorithms are supervised learning methods to split data into classes. They can work on Linear Data as well as Nonlinear Data. Logistic Regression can classify data based on weighted parameters and sigmoid conversion to calculate the probability of classes. K-nearest Neighbors (KNN) algorithm uses similar features to classify data. Support Vector Machines (SVMs) classify data by detecting the maximum margin hyperplane between data classes. Na\u00efve Bayes, a simplified Bayes Model, can help classify data using conditional probability models. Decision Trees are powerful classifiers and use tree splitting logic until pure or somewhat pure leaf node classes are attained. Random Forests apply Ensemble Learning to Decision Trees for more accurate classification predictions. Conclusion This completes \u2018Classification\u2019 tutorial. In the next tutorial, we will learn 'Unsupervised Learning with Clustering.'",
            "url": "https://github.com/sayantann11/all-classification-templetes-for-ML",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "Parkinson-Disease-Prediction",
            "description": "Introduction  Parkinson\u2019s Disease is the second most prevalent neurodegenerative disorder after Alzheimer\u2019s, affecting more than 10 million people worldwide. Parkinson\u2019s is characterized primarily by the deterioration of motor and cognitive ability. There is no single test which can be administered for diagnosis. Instead, doctors must perform a careful clinical analysis of the patient\u2019s medical history. Unfortunately, this method of diagnosis is highly inaccurate. A study from the National Institute of Neurological Disorders finds that early diagnosis (having symptoms for 5 years or less) is only 53% accurate. This is not much better than random guessing, but an early diagnosis is critical to effective treatment. Because of these difficulties, I investigate a machine learning approach to accurately diagnose Parkinson\u2019s, using a dataset of various speech features (a non-invasive yet characteristic tool) from the University of Oxford. Why speech features? Speech is very predictive and characteristic of Parkinson\u2019s disease; almost every Parkinson\u2019s patient experiences severe vocal degradation (inability to produce sustained phonations, tremor, hoarseness), so it makes sense to use voice to diagnose the disease. Voice analysis gives the added benefit of being non-invasive, inexpensive, and very easy to extract clinically. Background  Parkinson's Disease  Parkinson\u2019s is a progressive neurodegenerative condition resulting from the death of the dopamine containing cells of the substantia nigra (which plays an important role in movement). Symptoms include: \u201cfrozen\u201d facial features, bradykinesia (slowness of movement), akinesia (impairment of voluntary movement), tremor, and voice impairment. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. Performance Metrics  TP = true positive, FP = false positive, TN = true negative, FN = false negative Accuracy: (TP+TN)/(P+N) Matthews Correlation Coefficient: 1=perfect, 0=random, -1=completely inaccurate Algorithms Employed  Logistic Regression (LR): Uses the sigmoid logistic equation with weights (coefficient values) and biases (constants) to model the probability of a certain class for binary classification. An output of 1 represents one class, and an output of 0 represents the other. Training the model will learn the optimal weights and biases. Linear Discriminant Analysis (LDA): Assumes that the data is Gaussian and each feature has the same variance. LDA estimates the mean and variance for each class from the training data, and then uses properties of statistics (Bayes theorem , Gaussian distribution, etc) to compute the probability of a particular instance belonging to a given class. The class with the largest probability is the prediction. k Nearest Neighbors (KNN): Makes predictions about the validation set using the entire training set. KNN makes a prediction about a new instance by searching through the entire set to find the k \u201cclosest\u201d instances. \u201cCloseness\u201d is determined using a proximity measurement (Euclidean) across all features. The class that the majority of the k closest instances belong to is the class that the model predicts the new instance to be. Decision Tree (DT): Represented by a binary tree, where each root node represents an input variable and a split point, and each leaf node contains an output used to make a prediction. Neural Network (NN): Models the way the human brain makes decisions. Each neuron takes in 1+ inputs, and then uses an activation function to process the input with weights and biases to produce an output. Neurons can be arranged into layers, and multiple layers can form a network to model complex decisions. Training the network involves using the training instances to optimize the weights and biases. Naive Bayes (NB): Simplifies the calculation of probabilities by assuming that all features are independent of one another (a strong but effective assumption). Employs Bayes Theorem to calculate the probabilities that the instance to be predicted is in each class, then finds the class with the highest probability. Gradient Boost (GB): Generally used when seeking a model with very high predictive performance. Used to reduce bias and variance (\u201cerror\u201d) by combining multiple \u201cweak learners\u201d (not very good models) to create a \u201cstrong learner\u201d (high performance model). Involves 3 elements: a loss function (error function) to be optimized, a weak learner (decision tree) to make predictions, and an additive model to add trees to minimize the loss function. Gradient descent is used to minimize error after adding each tree (one by one). Engineering Goal  Produce a machine learning model to diagnose Parkinson\u2019s disease given various features of a patient\u2019s speech with at least 90% accuracy and/or a Matthews Correlation Coefficient of at least 0.9. Compare various algorithms and parameters to determine the best model for predicting Parkinson\u2019s.  Dataset Description  Source: the University of Oxford 195 instances (147 subjects with Parkinson\u2019s, 48 without Parkinson\u2019s) 22 features (elements that are possibly characteristic of Parkinson\u2019s, such as frequency, pitch, amplitude / period of the sound wave) 1 label (1 for Parkinson\u2019s, 0 for no Parkinson\u2019s) Project Pipeline  pipeline  Summary of Procedure  Split the Oxford Parkinson\u2019s Dataset into two parts: one for training, one for validation (evaluate how well the model performs) Train each of the following algorithms with the training set: Logistic Regression, Linear Discriminant Analysis, k Nearest Neighbors, Decision Tree, Neural Network, Naive Bayes, Gradient Boost Evaluate results using the validation set Repeat for the following training set to validation set splits: 80% training / 20% validation, 75% / 25%, and 70% / 30% Repeat for a rescaled version of the dataset (scale all the numbers in the dataset to a range from 0 to 1: this helps to reduce the effect of outliers) Conduct 5 trials and average the results Data  a_o  a_r  m_o  m_r  Data Analysis  In general, the models tended to perform the best (both in terms of accuracy and Matthews Correlation Coefficient) on the rescaled dataset with a 75-25 train-test split. The two highest performing algorithms, k Nearest Neighbors and the Neural Network, both achieved an accuracy of 98%. The NN achieved a MCC of 0.96, while KNN achieved a MCC of 0.94. These figures outperform most existing literature and significantly outperform current methods of diagnosis. Conclusion and Significance  These robust results suggest that a machine learning approach can indeed be implemented to significantly improve diagnosis methods of Parkinson\u2019s disease. Given the necessity of early diagnosis for effective treatment, my machine learning models provide a very promising alternative to the current, rather ineffective method of diagnosis. Current methods of early diagnosis are only 53% accurate, while my machine learning model produces 98% accuracy. This 45% increase is critical because an accurate, early diagnosis is needed to effectively treat the disease. Typically, by the time the disease is diagnosed, 60% of nigrostriatal neurons have degenerated, and 80% of striatal dopamine have been depleted. With an earlier diagnosis, much of this degradation could have been slowed or treated. My results are very significant because Parkinson\u2019s affects over 10 million people worldwide who could benefit greatly from an early, accurate diagnosis. Not only is my machine learning approach more accurate in terms of diagnostic accuracy, it is also more scalable, less expensive, and therefore more accessible to people who might not have access to established medical facilities and professionals. The diagnosis is also much simpler, requiring only a 10-15 second voice recording and producing an immediate diagnosis. Future Research  Given more time and resources, I would investigate the following: Create a mobile application which would allow the user to record his/her voice, extract the necessary vocal features, and feed it into my machine learning model to diagnose Parkinson\u2019s. Use larger datasets in conjunction with the University of Oxford dataset. Tune and improve my models even further to achieve even better results. Investigate different structures and types of neural networks. Construct a novel algorithm specifically suited for the prediction of Parkinson\u2019s. Generalize my findings and algorithms for all types of dementia disorders, such as Alzheimer\u2019s. References  Bind, Shubham. \"A Survey of Machine Learning Based Approaches for Parkinson Disease Prediction.\" International Journal of Computer Science and Information Technologies 6 (2015): n. pag. International Journal of Computer Science and Information Technologies. 2015. Web. 8 Mar. 2017. Brooks, Megan. \"Diagnosing Parkinson's Disease Still Challenging.\" Medscape Medical News. National Institute of Neurological Disorders, 31 July 2014. Web. 20 Mar. 2017. Exploiting Nonlinear Recurrence and Fractal Scaling Properties for Voice Disorder Detection', Little MA, McSharry PE, Roberts SJ, Costello DAE, Moroz IM. BioMedical Engineering OnLine 2007, 6:23 (26 June 2007) Hashmi, Sumaiya F. \"A Machine Learning Approach to Diagnosis of Parkinson\u2019s Disease.\"Claremont Colleges Scholarship. Claremont College, 2013. Web. 10 Mar. 2017. Karplus, Abraham. \"Machine Learning Algorithms for Cancer Diagnosis.\" Machine Learning Algorithms for Cancer Diagnosis (n.d.): n. pag. Mar. 2012. Web. 20 Mar. 2017. Little, Max. \"Parkinsons Data Set.\" UCI Machine Learning Repository. University of Oxford, 26 June 2008. Web. 20 Feb. 2017. Ozcift, Akin, and Arif Gulten. \"Classifier Ensemble Construction with Rotation Forest to Improve Medical Diagnosis Performance of Machine Learning Algorithms.\" Computer Methods and Programs in Biomedicine 104.3 (2011): 443-51. Semantic Scholar. 2011. Web. 15 Mar. 2017. \"Parkinson\u2019s Disease Dementia.\" UCI MIND. N.p., 19 Oct. 2015. Web. 17 Feb. 2017. Salvatore, C., A. Cerasa, I. Castiglioni, F. Gallivanone, A. Augimeri, M. Lopez, G. Arabia, M. Morelli, M.c. Gilardi, and A. Quattrone. \"Machine Learning on Brain MRI Data for Differential Diagnosis of Parkinson's Disease and Progressive Supranuclear Palsy.\"Journal of Neuroscience Methods 222 (2014): 230-37. 2014. Web. 18 Mar. 2017. Shahbakhi, Mohammad, Danial Taheri Far, and Ehsan Tahami. \"Speech Analysis for Diagnosis of Parkinson\u2019s Disease Using Genetic Algorithm and Support Vector Machine.\"Journal of Biomedical Science and Engineering 07.04 (2014): 147-56. Scientific Research. July 2014. Web. 2 Mar. 2017. \"Speech and Communication.\" Speech and Communication. Parkinson's Disease Foundation, n.d. Web. 22 Mar. 2017. Sriram, Tarigoppula V. S., M. Venkateswara Rao, G. V. Satya Narayana, and D. S. V. G. K. Kaladhar. \"Diagnosis of Parkinson Disease Using Machine Learning and Data Mining Systems from Voice Dataset.\" SpringerLink. Springer, Cham, 01 Jan. 1970. Web. 17 Mar. 2017.",
            "url": "https://github.com/Aastha2104/Parkinson-Disease-Prediction",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Understand the working principle of decision trees.",
          "Implement Random Forest and Gradient Boosting.",
          "Compare the performance of different algorithms."
        ],
        "estimated_hours": 9.0,
        "deadline": "2024-11-17 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      },
      {
        "week_number": 5,
        "title": "Model Evaluation and Hyperparameter Tuning",
        "description": "Learning how to evaluate models and optimize their performance.",
        "resources": [
          {
            "title": "Hyperparameter Tuning in Machine Learning: GridSearchCV &amp; RandomizedSearchCV | Dev DuniyaML Tutorial",
            "description": "ML Blog Tutorial: https://devduniya.com/hyperparameter-tuning-gridsearchcv-and-randomizedsearchcv-in-machine-learning/ ...",
            "url": "https://www.youtube.com/watch?v=m3kDJCWuoTg",
            "resource_type": "youtube_video",
            "duration": "PT18M7S",
            "difficulty": null,
            "tags": [
              "Hyperparameter tuning in machine learning",
              "RandomizedSearchCV explained",
              "Machine learning model optimization",
              "Fine-tuning hyperparameters in ML",
              "Advanced machine learning techniques",
              "Boosting model performance",
              "Hyperparameter optimization methods",
              "Python GridSearchCV tutorial",
              "RandomizedSearchCV implementation",
              "Optimize machine learning algorithms",
              "Hyperparameter search strategies",
              "Machine learning best practices",
              "Hyperparameter tuning techniques",
              "DevDuniya Machine Learning tutori"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Hyper parameter tuning||Malayalam||Machine Learning Course||Part-27",
            "description": "Hyper parameter tuning||Malayalam||Machine Learning Course||Part-27 This part of \"Machine Learning Course\" in Malayalam ...",
            "url": "https://www.youtube.com/watch?v=XIDw710t3MU",
            "resource_type": "youtube_video",
            "duration": "PT5M1S",
            "difficulty": null,
            "tags": [
              "artificial intelligence malayalam",
              "machine learning",
              "machine learning malayalam",
              "artificial intelligence",
              "Malayalam",
              "brAIn Tek",
              "gridsearchcv",
              "RandomizedSearchCV",
              "hyperparameter tuning machine learning",
              "hyperparameter tuning",
              "hyperparameter",
              "hyperparameter optimization",
              "hyperparameters",
              "hyperparameter tuning python",
              "grid search",
              "grid search machine learning",
              "grid search cv",
              "grid search python",
              "parameter tuning",
              "artificial intelligence in malayalam"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "8.3. Hyperparameter Tuning - GridSearchCV and RandomizedSearchCV",
            "description": "Hi! I will be conducting one-on-one discussion with all channel members. Checkout the perks and Join membership if interested: ...",
            "url": "https://www.youtube.com/watch?v=DTcfH5W6o08",
            "resource_type": "youtube_video",
            "duration": "PT13M36S",
            "difficulty": null,
            "tags": [
              "hyperparameter tuning",
              "GridSearchCV",
              "RandomizedSearchCV",
              "what is hyperparameter tuning",
              "what is GridSearchCV",
              "What is RandomizedSearchCV",
              "Grid Search CV",
              "Randomized Search CV",
              "Machine Learning",
              "Machine Learning tutorial",
              "Machine Learning course",
              "Machine Learning training"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Machine-Learning-RUL",
            "description": "This project predicts the Remaining Useful Life (RUL) of batteries using a Random Forest Regressor. It involves data preprocessing, feature selection through Sequential Forward Selection, and hyperparameter tuning with GridSearchCV. The model's performance is evaluated using R-squared and RMSE, and key insights are visualized to ensure reliability.",
            "url": "https://github.com/ServeshSingaravelan/Machine-Learning-RUL",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          },
          {
            "title": "Income-Predictor",
            "description": "\ud83d\udd0d\u2728 A machine learning project that predicts income based on various demographic factors using Random Forest and Gradient Boosting algorithms. Includes data preprocessing, hyperparameter tuning, and model evaluation with detailed performance metrics. \ud83d\udcca\ud83e\udd16",
            "url": "https://github.com/Armanx200/Income-Predictor",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [
              "arman-kianian",
              "classification",
              "data-preprocessing",
              "data-science",
              "gradient-boosting",
              "hyperparameter-tuning",
              "income-prediction",
              "machine-learning",
              "python",
              "random-forest",
              "scikit-learn"
            ],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Understand various model evaluation metrics.",
          "Implement techniques for hyperparameter tuning.",
          "Improve model performance through optimization."
        ],
        "estimated_hours": 7.0,
        "deadline": "2024-11-24 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      },
      {
        "week_number": 6,
        "title": "Project: Building a Machine Learning Model",
        "description": "Apply what you've learned to build a complete machine learning model.",
        "resources": [
          {
            "title": "Machine Learning | What Is Machine Learning? | Introduction To Machine Learning | 2024 | Simplilearn",
            "description": "Artificial Intelligence Engineer (IBM) ...",
            "url": "https://www.youtube.com/watch?v=ukzFI9rgwfU",
            "resource_type": "youtube_video",
            "duration": "PT7M52S",
            "difficulty": null,
            "tags": [
              "2022",
              "2023",
              "ML",
              "Machine Learning",
              "basics of machine learning",
              "cloud and machine learning",
              "fundamentals of machine learning",
              "gcp maching learning",
              "getting started with machine learning",
              "introduction to machine learning",
              "machine learning",
              "machine learning basics",
              "machine learning for beginners",
              "machine learning tutorial for beginners",
              "machine learning with gcp",
              "simplilearn",
              "simplilearn machine learning",
              "what is machine learning"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "All Machine Learning Models Explained in 5 Minutes | Types of ML Models Basics",
            "description": "Get Certified in Artificial Intelligence & Machine Learning. Both tech and Non-Tech can apply! 10% off on AI Certifications.",
            "url": "https://www.youtube.com/watch?v=yN7ypxC7838",
            "resource_type": "youtube_video",
            "duration": "PT5M1S",
            "difficulty": null,
            "tags": [
              "machine learning models",
              "machine learning models explained",
              "machine learning",
              "ml models explained",
              "machine learning models basics",
              "example",
              "supervised learning",
              "machine learning models for beginners",
              "machine learning models overview",
              "machine learning models from scratch",
              "ml models basics",
              "regression",
              "unsupervised learning",
              "classification",
              "beginners",
              "basics",
              "overview",
              "types of ml models",
              "basics of machine learning models"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "AI, Machine Learning, Deep Learning and Generative AI Explained",
            "description": "Want to learn about AI agents and assistants? Register for Virtual Agents Day here \u2192 https://ibm.biz/BdaAVa Want to play with the ...",
            "url": "https://www.youtube.com/watch?v=qYNweeDHiyU",
            "resource_type": "youtube_video",
            "duration": "PT10M1S",
            "difficulty": null,
            "tags": [
              "IBM",
              "IBM Cloud",
              "AI",
              "MachineLearning",
              "DeepLearning",
              "GenerativeAI",
              "GenAI",
              "Gen AI",
              "LargeLanguageModels",
              "LLMs",
              "Deepfakes",
              "Deep Fakes",
              "TechExplained",
              "ArtificialIntelligence",
              "Artificial Intelligence",
              "ML",
              "DL",
              "AIExplained",
              "TechTrends",
              "DataScience",
              "NeuralNetworks",
              "Chatbots",
              "FutureOfAI",
              "TechSimplified"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones",
            "description": "This was my Master's project where i was involved using a dataset from Wireless Sensor Data Mining Lab (WISDM) to build a machine learning model to predict basic human activities using a smartphone accelerometer, Using Tensorflow framework, recurrent neural nets and multiple stacks of Long-short-term memory units(LSTM)  for building a deep network.  After the model was trained,  it was saved and exported to an android application and the predictions were made using the model and the interface to speak out the results using text-to-speech API.",
            "url": "https://github.com/girishp92/Human-activity-recognition-using-Recurrent-Neural-Nets-RNN-LSTM-and-Tensorflow-on-Smartphones",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [
              "android-application",
              "androidstudio",
              "deep-learning",
              "hidden-units",
              "human-activities",
              "lstm-neural-networks",
              "machine-learning",
              "pickle",
              "proof-of-concept",
              "pycharm-ide",
              "python3",
              "rnn-tensorflow",
              "scikit-learn",
              "smartphone-accelerometer"
            ],
            "estimated_completion_time": null
          },
          {
            "title": "Training-Targets-for-Speech-Separation-Neural-Networks",
            "description": "This is a project on working/resolving the speech separation problem using supervised learning on various training targets, building machine learning model using feed forward neural networks. Implementing metrics like STOI and PESQ for speech quality and interpretability metrics.",
            "url": "https://github.com/jaideeppatel/Training-Targets-for-Speech-Separation-Neural-Networks",
            "resource_type": "github_project",
            "duration": null,
            "difficulty": null,
            "tags": [],
            "estimated_completion_time": null
          }
        ],
        "objectives": [
          "Choose a dataset and formulate a problem statement.",
          "Clean and preprocess the data.",
          "Train, evaluate, and tune a suitable machine learning model.",
          "Document the process and results."
        ],
        "estimated_hours": 10.0,
        "deadline": "2024-12-01 00:00:00",
        "completed": false,
        "progress_percentage": 0.0
      }
    ],
    "created_at": "2025-07-20 12:49:28.486529",
    "last_updated": "2025-07-20 12:49:28.486529",
    "overall_progress": 0.0
  },
  "progress_updates": [],
  "adaptive_recommendations": [],
  "created_at": "2025-07-20 12:49:28.486529",
  "last_updated": "2025-07-20 12:49:28.486529"
}